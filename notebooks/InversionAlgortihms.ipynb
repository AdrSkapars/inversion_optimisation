{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1328932",
   "metadata": {},
   "source": [
    "# Inversion Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d718043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For when connected to google colab\n",
    "!git clone https://github.com/AdrSkapars/inversion_optimisation.git \n",
    "%cd inversion_optimisation\n",
    "!uv pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdab265",
   "metadata": {},
   "source": [
    "## Set Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f126c3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/inversion_optimisation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/content/inversion_optimisation/src\")\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from inversion_optimisation.utils import (\n",
    "    get_paper_summary_stats_new, \n",
    "    DotDict,\n",
    "    DATA_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bad4a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-33M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Load the LLM model\n",
    "\n",
    "device = [\"cuda\", \"cpu\"][0]\n",
    "model_name = {\n",
    "    # Simple models\n",
    "    0: \"attn-only-1l\",\n",
    "    1: \"gelu-1l\",\n",
    "    2: \"tiny-stories-1L-21M\",\n",
    "    \n",
    "    ## Small models\n",
    "    3: \"tiny-stories-1M\",\n",
    "    4: \"tiny-stories-3M\",\n",
    "    5: \"tiny-stories-8M\",\n",
    "    6: \"tiny-stories-28M\",\n",
    "    7: \"tiny-stories-33M\",\n",
    "    8: \"tiny-stories-instruct-33M\",\n",
    "    \n",
    "    ## Large models\n",
    "    9: \"gpt2-small\",\n",
    "    10: \"gpt2-medium\",\n",
    "    11: \"gpt2-xl\",\n",
    "    12: \"llama-7b\",\n",
    "    13: \"meta-llama/Llama-3.2-1B\",\n",
    "    14: \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    15: \"Qwen/Qwen2.5-0.5B\",\n",
    "    16: \"Qwen/Qwen2.5-1.5B\",\n",
    "    17: \"Qwen/Qwen2.5-3B\",\n",
    "    18: \"pythia-1.4b\",\n",
    "}[7]\n",
    "\n",
    "model = HookedTransformer.from_pretrained(model_name)#, device=device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369ac165",
   "metadata": {},
   "source": [
    "# ------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139d409",
   "metadata": {},
   "source": [
    "## SODA Text-Only Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c04cd061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_optimisation.algorithms.soda import onehot_text_search\n",
    "from inversion_optimisation.inv_configs import SODA_TEXT_CFG\n",
    "\n",
    "cfg = SODA_TEXT_CFG\n",
    "# cfg = DotDict({\n",
    "#     \"learn_rate\" : 0.065,\n",
    "#     \"decay_rate\" : 0.9,\n",
    "#     \"betas\" : (0.9,0.995),\n",
    "#     \"temp\" : 0.05,\n",
    "#     \"reset_epoch\" : 50,\n",
    "#     \"reinit_epoch\" : 1500,\n",
    "#     \"reg_weight\" : None,#9e-3,\n",
    "#     \"bias_correction\" : False,\n",
    "#     \"target_sample\" : {\n",
    "#         0: \"random\",\n",
    "#         1: \"greedy\"\n",
    "#     }[1],\n",
    "#     # \"target_strategy\" : {\n",
    "#     #     0: \"random\",\n",
    "#     #     1: \"tinystories\",\n",
    "#     #     2: \"reddit\",\n",
    "#     #     3: \"wikipedia\"\n",
    "#     # }[0],\n",
    "#     \"init_strategy\" : {\n",
    "#         0: \"loaded\",\n",
    "#         1: \"normal\",\n",
    "#         2: \"zeros\",\n",
    "#     }[2],\n",
    "#     \"save_folder\": \"OneHot_TinyStories33M_text\",\n",
    "#     \"model_name\": \"tiny-stories-33M\",\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for dataset size\n",
    "cfg.input_len = 1\n",
    "cfg.output_len = 25\n",
    "cfg.num_targets = 1000\n",
    "cfg.max_epochs = 100 * 1000\n",
    "cfg.max_batch_size = {\n",
    "    \"tiny-stories-33M\" : [300,300,280,270,260,250,250,240,240,230],\n",
    "}[cfg.model_name][cfg.input_len-1]\n",
    "cfg.max_batch_size = cfg.max_batch_size - 15\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Generate the targets used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'true_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "batch_size = 1000\n",
    "for batch in range(0, cfg.num_targets, batch_size):\n",
    "    input_tokens = true_tokens[batch:batch+batch_size].to(device)\n",
    "    output_tokens = model.generate(\n",
    "        input_tokens,\n",
    "        # min_new_tokens=cfg.output_len,\n",
    "        max_new_tokens=cfg.output_len,\n",
    "        # do_sample=True,\n",
    "        do_sample=False,\n",
    "        stop_at_eos=False,\n",
    "        verbose=False,\n",
    "        return_type=\"tokens\",)[:,cfg.input_len:]\n",
    "    if batch == 0:\n",
    "        all_output_tokens = output_tokens\n",
    "    else:\n",
    "        all_output_tokens = torch.cat((all_output_tokens, output_tokens), dim=0)\n",
    "with open(DATA_PATH / f\"true_tokens_{cfg.num_targets}_{cfg.input_len}_{cfg.output_len}_greedy.pkl\", 'wb') as file:\n",
    "    pickle.dump(all_output_tokens, file)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# # Generate the initialisations used for all experiments\n",
    "# tokens_list = []\n",
    "# for _ in tqdm(range(cfg.num_targets)):\n",
    "#     tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "#     tokens_list.append(tokens)\n",
    "# initial_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "# with open(DATA_PATH / f'initial_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "#     pickle.dump(initial_tokens, file)\n",
    "\n",
    "results, elapsed_time = onehot_text_search(cfg, model, device)\n",
    "print()\n",
    "stats = get_paper_summary_stats_new(results, epochs=cfg.max_epochs)\n",
    "stats[\"elapsed_time\"] = elapsed_time\n",
    "stats[\"experiment_params\"] = cfg\n",
    "print(\"input_len\", cfg.input_len, \"\\tpercent_zero_loss\", stats[\"percent_zero_loss\"], \"\\tpercent_exact_inversion\", stats[\"percent_exact_inversion\"], \"\\telapsed_time\", stats[\"elapsed_time\"])\n",
    "\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/stats_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/results_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1374d115",
   "metadata": {},
   "source": [
    "## SODA Logits Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec2f4e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_optimisation.algorithms.soda import onehot_search\n",
    "from inversion_optimisation.inv_configs import SODA_LOGITS_CFG, SODA_LOGITS_CFG_DIFF_MODELS\n",
    "\n",
    "cfg = SODA_LOGITS_CFG\n",
    "# cfg = SODA_LOGITS_CFG_DIFF_MODELS[cfg.model_name]\n",
    "# cfg = DotDict({\n",
    "#     \"learn_rate\" : 0.065,\n",
    "#     \"decay_rate\" : 0.9,\n",
    "#     \"betas\" : (0.9,0.995),\n",
    "#     \"temp\" : 0.05,\n",
    "#     \"reset_epoch\" : 50,\n",
    "#     \"reinit_epoch\" : 1500,\n",
    "#     \"reg_weight\" : None,#9e-3,\n",
    "#     \"bias_correction\" : False,\n",
    "#     # \"target_strategy\" : {\n",
    "#     #     0: \"random\",\n",
    "#     #     1: \"tinystories\",\n",
    "#     #     2: \"reddit\",\n",
    "#     #     3: \"wikipedia\",\n",
    "#     #     4: \"privacy\",\n",
    "#     # }[0],\n",
    "#     \"init_strategy\" : {\n",
    "#         0: \"loaded\",\n",
    "#         1: \"normal\",\n",
    "#         2: \"zeros\",\n",
    "#     }[2],\n",
    "#     \"save_folder\": \"OneHot_TinyStories33M\",\n",
    "#     \"model_name\": \"tiny-stories-33M\",\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19d3b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:12<00:00, 77.69it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "onehot_search() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     28\u001b[39m random.seed(\u001b[32m42\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# # Generate the initialisations used for all experiments\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# tokens_list = []\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# for _ in tqdm(range(cfg.num_targets)):\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# with open(f'data/initial_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m#     pickle.dump(initial_tokens, file)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m results, elapsed_time = \u001b[43monehot_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     41\u001b[39m stats = get_paper_summary_stats_new(results, epochs=cfg.max_epochs)\n",
      "\u001b[31mTypeError\u001b[39m: onehot_search() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "# Set parameters for dataset size\n",
    "cfg.input_len = 1\n",
    "cfg.num_targets = 1000\n",
    "cfg.max_epochs = 100# * 1000\n",
    "cfg.max_batch_size = {\n",
    "    \"tiny-stories-33M\" : [1000,1000,1000,1000,1000,1000,1000,900,750,650,None, None, None, None, 500, 500, 475, 450, 410, 380, 375, 350, 325, 300],\n",
    "    \"gpt2-small\": [1000,1000,1000,1000,1000,950,850,700,650,600],\n",
    "    \"Qwen/Qwen2.5-0.5B\": [1000,700,500,400,300,240,220,200,180,100],\n",
    "}[cfg.model_name][cfg.input_len-1]\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Generate the targets used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'true_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# # Generate the initialisations used for all experiments\n",
    "# tokens_list = []\n",
    "# for _ in tqdm(range(cfg.num_targets)):\n",
    "#     tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "#     tokens_list.append(tokens)\n",
    "# initial_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "# with open(DATA_PATH / f'initial_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "#     pickle.dump(initial_tokens, file)\n",
    "\n",
    "results, elapsed_time = onehot_search(cfg, model, device)\n",
    "print()\n",
    "stats = get_paper_summary_stats_new(results, epochs=cfg.max_epochs)\n",
    "stats[\"elapsed_time\"] = elapsed_time\n",
    "stats[\"experiment_params\"] = cfg\n",
    "print(\"input_len\", cfg.input_len, \"\\tpercent_zero_loss\", stats[\"percent_zero_loss\"], \"\\tpercent_exact_inversion\", stats[\"percent_exact_inversion\"], \"\\telapsed_time\", stats[\"elapsed_time\"])\n",
    "\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/stats_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/results_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215cf20d",
   "metadata": {},
   "source": [
    "## GCG Text-Only Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac09ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_optimisation.algorithms.gcg import gcg_text_search\n",
    "from inversion_optimisation.inv_configs import GCG_TEXT_CFG\n",
    "\n",
    "cfg = GCG_TEXT_CFG\n",
    "# cfg = DotDict({\n",
    "#     \"top_k\" : 128,\n",
    "#     \"pos_choice\" : {\n",
    "#         0: \"uniform\",\n",
    "#         1: \"weighted\",\n",
    "#         2: \"greedy\",\n",
    "#     }[0],\n",
    "#     \"token_choice\" : {\n",
    "#         0: \"uniform\",\n",
    "#         1: \"weighted\",\n",
    "#     }[0],\n",
    "#     \"num_mutations\" : 1,\n",
    "#     \"target_sample\" : {\n",
    "#         0: \"random\",\n",
    "#         1: \"greedy\"\n",
    "#     }[1],\n",
    "#     # \"target_strategy\" : {\n",
    "#     #     0: \"random\",\n",
    "#     #     1: \"tinystories\",\n",
    "#     #     2: \"reddit\",\n",
    "#     #     3: \"wikipedia\"\n",
    "#     # }[0],\n",
    "#     \"init_strategy\" : {\n",
    "#         0: \"loaded\",\n",
    "#         1: \"zeros\",\n",
    "#     }[0],\n",
    "#     \"save_folder\": \"GCG_TinyStories33M_text\",\n",
    "#     \"model_name\": \"tiny-stories-33M\",\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for dataset size\n",
    "cfg.output_len = 25\n",
    "cfg.input_len = 1\n",
    "cfg.num_targets = 1000\n",
    "cfg.num_candidates = (32*22)\n",
    "cfg.adjusted_max_epochs = (32*22)\n",
    "cfg.max_batch_size = {\n",
    "    \"tiny-stories-33M\" : [300,300,280,270,260,250,250,240,240,230],\n",
    "}[cfg.model_name][cfg.input_len-1]\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Generate the targets and initialisations used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'true_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "batch_size = 1000\n",
    "for batch in range(0, cfg.num_targets, batch_size):\n",
    "    input_tokens = true_tokens[batch:batch+batch_size].to(device)\n",
    "    output_tokens = model.generate(\n",
    "        input_tokens,\n",
    "        max_new_tokens=cfg.output_len,\n",
    "        do_sample=False,\n",
    "        stop_at_eos=False,\n",
    "        verbose=False,\n",
    "        return_type=\"tokens\",)[:,cfg.input_len:]\n",
    "    if batch == 0:\n",
    "        all_output_tokens = output_tokens\n",
    "    else:\n",
    "        all_output_tokens = torch.cat((all_output_tokens, output_tokens), dim=0)\n",
    "with open(DATA_PATH / f\"true_tokens_{cfg.num_targets}_{cfg.input_len}_{cfg.output_len}_greedy.pkl\", 'wb') as file:\n",
    "    pickle.dump(all_output_tokens, file)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f\"initial_tokens_{cfg.num_targets}_{cfg.input_len}.pkl\", 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "results, elapsed_time = gcg_text_search(cfg, model, device)\n",
    "print()\n",
    "stats = get_paper_summary_stats_new(results, epochs=cfg.adjusted_max_epochs)\n",
    "stats[\"elapsed_time\"] = elapsed_time\n",
    "stats[\"experiment_params\"] = cfg\n",
    "print(\"input_len\", cfg.input_len, \"\\tpercent_zero_loss\", stats[\"percent_zero_loss\"], \"\\tpercent_exact_inversion\", stats[\"percent_exact_inversion\"], \"\\telapsed_time\", stats[\"elapsed_time\"])\n",
    "\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/stats_{cfg.input_len}_{cfg.num_targets}_{cfg.adjusted_max_epochs}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/results_{cfg.input_len}_{cfg.num_targets}_{cfg.adjusted_max_epochs}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94c189",
   "metadata": {},
   "source": [
    "## GCG Logits Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a0a75d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inversion_optimisation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minversion_optimisation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgcg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gcg_search\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minversion_optimisation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minv_configs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GCG_LOGITS_CFG\n\u001b[32m      4\u001b[39m cfg = GCG_LOGITS_CFG\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'inversion_optimisation'"
     ]
    }
   ],
   "source": [
    "from inversion_optimisation.algorithms.gcg import gcg_search\n",
    "from inversion_optimisation.inv_configs import GCG_LOGITS_CFG\n",
    "\n",
    "cfg = GCG_LOGITS_CFG\n",
    "cfg = DotDict({\n",
    "    \"top_k\" : 128,\n",
    "    \"pos_choice\" : {\n",
    "        0: \"uniform\",\n",
    "        1: \"weighted\",\n",
    "        2: \"greedy\",\n",
    "    }[0],\n",
    "    \"token_choice\" : {\n",
    "        0: \"uniform\",\n",
    "        1: \"weighted\",\n",
    "    }[0],\n",
    "    \"num_mutations\" : 1,\n",
    "    # \"target_strategy\" : {\n",
    "    #     0: \"random\",\n",
    "    #     1: \"tinystories\",\n",
    "    #     2: \"reddit\",\n",
    "    #     3: \"wikipedia\",\n",
    "    #     4: \"privacy\",\n",
    "    # }[0],\n",
    "    \"init_strategy\" : {\n",
    "        0: \"loaded\",\n",
    "        1: \"zeros\",\n",
    "    }[0],\n",
    "    \"save_folder\": \"GCG_TinyStories33M\",\n",
    "    \"model_name\": \"tiny-stories-33M\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for dataset size\n",
    "cfg.input_len = 15\n",
    "cfg.num_targets = 1000\n",
    "cfg.num_candidates = (32*22)\n",
    "cfg.adjusted_max_epochs = (32*22)\n",
    "cfg.max_batch_size = {\n",
    "    \"tiny-stories-33M\" : [1000,1000,1000,1000,1000,1000,1000,900,750,650,None, None, None, None, 500, 500, 475, 450, 420, 400, 375, 350, 325, 300],\n",
    "}[cfg.model_name][cfg.input_len-1]\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Generate the targets and initialisations used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'true_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f\"initial_tokens_{cfg.num_targets}_{cfg.input_len}.pkl\", 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "results, elapsed_time = gcg_search(cfg, model, device)\n",
    "print()\n",
    "stats = get_paper_summary_stats_new(results, epochs=cfg.adjusted_max_epochs)\n",
    "stats[\"elapsed_time\"] = elapsed_time\n",
    "stats[\"experiment_params\"] = cfg\n",
    "print(\"input_len\", cfg.input_len, \"\\tpercent_zero_loss\", stats[\"percent_zero_loss\"], \"\\tpercent_exact_inversion\", stats[\"percent_exact_inversion\"], \"\\telapsed_time\", stats[\"elapsed_time\"])\n",
    "\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/stats_{cfg.input_len}_{cfg.num_targets}_{cfg.adjusted_max_epochs}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/results_{cfg.input_len}_{cfg.num_targets}_{cfg.adjusted_max_epochs}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed7eb4",
   "metadata": {},
   "source": [
    "## Inv. Model Text-Only Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_optimisation.algorithms.text_inv_model import (\n",
    "    LLMInversionModel, \n",
    "    train_inversion_model, \n",
    "    test_inversion_model_dataset, \n",
    "    test_inversion_model\n",
    ")\n",
    "from inversion_optimisation.inv_configs import INV_MODEL_TEXT_CFG\n",
    "\n",
    "cfg = INV_MODEL_TEXT_CFG\n",
    "# cfg = DotDict({\n",
    "#     \"t5_model_name\" : \"t5-small\",\n",
    "#     \"t5_tokenizer_name\" : \"t5-small\",\n",
    "#     \"llm_model_name\" : \"roneneldan/TinyStories-33M\",\n",
    "#     \"num_generation_tokens\" : 25,\n",
    "#     \"seed\" : 24,\n",
    "#     \"dataset_size\" : 400000,\n",
    "#     \"min_seq_length\" : 1,\n",
    "#     \"max_seq_length\" : 10,\n",
    "#     \"max_length\" : 16,\n",
    "#     \"val_split\" : 0.1,\n",
    "#     # \"output_dir\" : \"/content/TextInvModel_Saves\",\n",
    "#     \"batch_size\" : 160,\n",
    "#     \"mini_batch_size\" : 160,\n",
    "#     \"num_epochs\" : 30,\n",
    "#     \"save_steps\" : 3000000,\n",
    "#     \"warmup_steps\" : 1000,\n",
    "#     \"num_workers\" : 12,\n",
    "#     \"learning_rate\" : 1e-3,\n",
    "#     \"weight_decay\" : 0.05,\n",
    "#     # \"dataset\" : [\"random\", \"reddit\", \"tinystories\"][0],\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load initial model\n",
    "inv_model = LLMInversionModel(\n",
    "    t5_model_name=cfg.t5_model_name,\n",
    "    t5_tokenizer_name=cfg.t5_tokenizer_name,\n",
    "    llm_model_name=cfg.llm_model_name,\n",
    "    num_generation_tokens=cfg.num_generation_tokens,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11957d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set level of logging\n",
    "# logging.getLogger().setLevel(logging.INFO)\n",
    "# logging.getLogger().setLevel(logging.WARNING)\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "elapsed_time = time.time()\n",
    "inv_model = train_inversion_model(\n",
    "    model=inv_model,\n",
    "    seed=cfg.seed, # Set random seed\n",
    "    dataset_size=cfg.dataset_size,\n",
    "    min_seq_length=cfg.min_seq_length,\n",
    "    max_seq_length=cfg.max_seq_length,\n",
    "    max_length=cfg.max_length,\n",
    "    val_split=cfg.val_split,\n",
    "    output_dir=\"data/TextInvModel_Saves\",\n",
    "    batch_size=cfg.batch_size,\n",
    "    mini_batch_size=cfg.mini_batch_size,\n",
    "    num_epochs=cfg.num_epochs,\n",
    "    save_steps=cfg.save_steps,\n",
    "    warmup_steps=cfg.warmup_steps,\n",
    "    num_workers=cfg.num_workers,\n",
    "    learning_rate=cfg.learning_rate,\n",
    "    weight_decay=cfg.weight_decay,\n",
    "    dataset=[\"random\", \"reddit\", \"tinystories\"][0],\n",
    ")\n",
    "elapsed_time = time.time() - elapsed_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37cd332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "num_targets = 1000\n",
    "eval_batch_size = 200\n",
    "input_len = 1\n",
    "max_length = 16\n",
    "\n",
    "# Load best model\n",
    "inv_model = LLMInversionModel.from_pretrained(\"data/TextInvModel_Saves/best_model\").to(device)\n",
    "\n",
    "# Generate dataset\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Choose random (Random) dataset\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(num_targets)):\n",
    "    tokens = torch.randint(0, len(inv_model.tokenizer.vocab), (1, input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "loaded_true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "loaded_text_samples = inv_model.llm_tokenizer.batch_decode(loaded_true_tokens)\n",
    "\n",
    "# # Choose reddit (NL OOD) dataset\n",
    "# loaded_true_tokens = load_dataset_tokens(\"reddit\", input_len, num_targets, include_bos=False, random_sentence=True, random_start=False)\n",
    "# loaded_text_samples = inv_model.llm_tokenizer.batch_decode(loaded_true_tokens)\n",
    "\n",
    "# # Choose tinystories (NL ID) dataset\n",
    "# loaded_true_tokens = load_dataset_tokens(\"tinystories\", input_len, num_targets, include_bos=False, random_sentence=True, random_start=False)\n",
    "# loaded_text_samples = inv_model.llm_tokenizer.batch_decode(loaded_true_tokens)\n",
    "\n",
    "# Run model\n",
    "results, stats = test_inversion_model_dataset(inv_model, loaded_text_samples, eval_batch_size, input_len, max_length)\n",
    "\n",
    "# with open(DATA_PATH / f'TextInvModel_Saves/stats_{input_len}_{num_targets}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'TextInvModel_Saves/results_{input_len}_{num_targets}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb58a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test model\n",
    "test_texts = [\"Hey\",\"Hello\",\"I\",\"So\", 'American']\n",
    "results = test_inversion_model(model, test_texts, max_length=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f05ad8",
   "metadata": {},
   "source": [
    "## Inv. Model Logits Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b77727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_optimisation.algorithms.inv_model import (\n",
    "    LLMInversionModel, \n",
    "    train_inversion_model, \n",
    "    test_inversion_model_dataset, \n",
    "    test_inversion_model\n",
    ")\n",
    "from inversion_optimisation.inv_configs import INV_MODEL_LOGITS_CFG\n",
    "\n",
    "cfg = INV_MODEL_LOGITS_CFG\n",
    "# cfg = DotDict({\n",
    "#     \"t5_model_name\" : \"t5-small\",\n",
    "#     \"t5_tokenizer_name\" : \"t5-small\",\n",
    "#     \"llm_model_name\" : \"roneneldan/TinyStories-33M\",\n",
    "#     \"unigram_beta\" : 0.01,\n",
    "#     \"num_tokens\" : 64,\n",
    "#     \"bottleneck_dim\" : 32768,\n",
    "#     \"seed\" : 24, # Set random seed\n",
    "#     \"dataset_size\" : 400000,\n",
    "#     \"min_seq_length\" : 1,\n",
    "#     \"max_seq_length\" : 10,\n",
    "#     \"max_length\" : 16,\n",
    "#     \"val_split\" : 0.1,\n",
    "#     # \"output_dir\" : \"/content/InvModel_Saves\",\n",
    "#     \"batch_size\" : 80,\n",
    "#     \"mini_batch_size\" : 80,\n",
    "#     \"num_epochs\" : 30,\n",
    "#     \"save_steps\" : 3000000,\n",
    "#     \"warmup_steps\" : 1000,\n",
    "#     \"num_workers\" : 12,\n",
    "#     \"learning_rate\" : 2e-4,\n",
    "#     \"weight_decay\" : 0.025,\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae908aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load initial model\n",
    "inv_model = LLMInversionModel(\n",
    "    t5_model_name=cfg.t5_model_name,\n",
    "    t5_tokenizer_name=cfg.t5_tokenizer_name,\n",
    "    llm_model_name=cfg.llm_model_name,\n",
    "    unigram_beta=cfg.unigram_beta,\n",
    "    num_tokens=cfg.num_tokens,\n",
    "    bottleneck_dim=cfg.bottleneck_dim,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set level of logging\n",
    "# logging.getLogger().setLevel(logging.INFO)\n",
    "# logging.getLogger().setLevel(logging.WARNING)\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "elapsed_time = time.time()\n",
    "inv_model = train_inversion_model(\n",
    "    model=inv_model,\n",
    "    seed=cfg.seed, # Set random seed\n",
    "    dataset_size=cfg.dataset_size,\n",
    "    min_seq_length=cfg.min_seq_length,\n",
    "    max_seq_length=cfg.max_seq_length,\n",
    "    max_length=cfg.max_length,\n",
    "    val_split=cfg.val_split,\n",
    "    output_dir=\"data/InvModel_Saves\",\n",
    "    batch_size=cfg.batch_size,\n",
    "    mini_batch_size=cfg.mini_batch_size,\n",
    "    num_epochs=cfg.num_epochs,\n",
    "    save_steps=cfg.save_steps,\n",
    "    warmup_steps=cfg.warmup_steps,\n",
    "    num_workers=cfg.num_workers,\n",
    "    learning_rate=cfg.learning_rate,\n",
    "    weight_decay=cfg.weight_decay,\n",
    ")\n",
    "elapsed_time = time.time() - elapsed_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "num_targets = 1000\n",
    "eval_batch_size = 200\n",
    "input_len = 1\n",
    "max_length = 32\n",
    "\n",
    "# Load best model\n",
    "inv_model = LLMInversionModel.from_pretrained(\"data/InvModel_Saves/best_model\").to(device)\n",
    "\n",
    "# Generate dataset\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(num_targets)):\n",
    "    tokens = torch.randint(0, len(inv_model.tokenizer.vocab), (1, input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "loaded_true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "loaded_text_samples = inv_model.llm_tokenizer.batch_decode(loaded_true_tokens)\n",
    "\n",
    "# Run model\n",
    "results, stats = test_inversion_model_dataset(inv_model, loaded_text_samples, eval_batch_size, input_len, max_length)\n",
    "\n",
    "# with open(DATA_PATH / f'InvModel_Saves/stats_{input_len}_{num_targets}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'InvModel_Saves/results_{input_len}_{num_targets}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test model\n",
    "test_texts = [\"Hey\",\"Hello\",\"I\",\"So\", 'American']\n",
    "results = test_inversion_model(model, test_texts, max_length=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a84853",
   "metadata": {},
   "source": [
    "# ------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c410b633",
   "metadata": {},
   "source": [
    "## Embed Search Text-Only Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac2807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_optimisation.algorithms.embed_search import embed_text_search\n",
    "\n",
    "cfg = DotDict({\n",
    "    \"learn_rate\" : 0.0003,\n",
    "    # \"decay_rate\" : 1,\n",
    "    \"betas\" : (0.9,0.999),\n",
    "    # \"reset_epoch\" : 999999,\n",
    "    # \"reinit_epoch\" : 999999,\n",
    "    # \"reg_weight\" : None,#9e-3,\n",
    "    # \"bias_correction\" : True,\n",
    "    \"con_distance\" : {\n",
    "        0: \"cos sim\",\n",
    "        1: \"euc dist\",\n",
    "    }[0],\n",
    "    \"target_sample\" : {\n",
    "        0: \"random\",\n",
    "        1: \"greedy\"\n",
    "    }[1],\n",
    "    \"target_strategy\" : {\n",
    "        0: \"random\",\n",
    "        1: \"tinystories\",\n",
    "        2: \"reddit\",\n",
    "        3: \"wikipedia\"\n",
    "    }[0],\n",
    "    \"init_strategy\" : {\n",
    "        0: \"loaded\",\n",
    "        1: \"normal\",\n",
    "        2: \"zeros\",\n",
    "    }[2],\n",
    "    \"save_folder\": \"Embed_TinyStories33M_text\",\n",
    "    \"model_name\": \"tiny-stories-33M\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e13bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for dataset size\n",
    "cfg.input_len = 1\n",
    "cfg.output_len = 25\n",
    "cfg.num_targets = 1000\n",
    "cfg.max_chunk_size = 20\n",
    "cfg.max_epochs = 100 * 1000\n",
    "cfg.max_batch_size = {\n",
    "    \"tiny-stories-33M\" : [300,300,280,270,260,250,250,240,240,230],\n",
    "}[cfg.model_name][cfg.input_len-1]\n",
    "cfg.max_batch_size = cfg.max_batch_size - 15\n",
    "cfg.check_con_epochs = 200\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Generate the targets used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'true_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "batch_size = 1000\n",
    "for batch in range(0, cfg.num_targets, batch_size):\n",
    "    input_tokens = true_tokens[batch:batch+batch_size].to(device)\n",
    "    output_tokens = model.generate(\n",
    "        input_tokens,\n",
    "        # min_new_tokens=cfg.output_len,\n",
    "        max_new_tokens=cfg.output_len,\n",
    "        # do_sample=True,\n",
    "        do_sample=False,\n",
    "        stop_at_eos=False,\n",
    "        verbose=False,\n",
    "        return_type=\"tokens\",)[:,cfg.input_len:]\n",
    "    if batch == 0:\n",
    "        all_output_tokens = output_tokens\n",
    "    else:\n",
    "        all_output_tokens = torch.cat((all_output_tokens, output_tokens), dim=0)\n",
    "with open(DATA_PATH / f\"true_tokens_{cfg.num_targets}_{cfg.input_len}_{cfg.output_len}_greedy.pkl\", 'wb') as file:\n",
    "    pickle.dump(all_output_tokens, file)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# # Generate the initialisations used for all experiments\n",
    "# tokens_list = []\n",
    "# for _ in tqdm(range(cfg.num_targets)):\n",
    "#     tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "#     tokens_list.append(tokens)\n",
    "# initial_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "# with open(DATA_PATH / f'initial_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "#     pickle.dump(initial_tokens, file)\n",
    "\n",
    "results, elapsed_time = embed_text_search(cfg, model, device)\n",
    "print()\n",
    "stats = get_paper_summary_stats_new(results, epochs=cfg.max_epochs)\n",
    "stats[\"elapsed_time\"] = elapsed_time\n",
    "stats[\"experiment_params\"] = cfg\n",
    "print(\"input_len\", cfg.input_len, \"\\tpercent_zero_loss\", stats[\"percent_zero_loss\"], \"\\tpercent_exact_inversion\", stats[\"percent_exact_inversion\"], \"\\telapsed_time\", stats[\"elapsed_time\"])\n",
    "\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/stats_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/results_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76311a5",
   "metadata": {},
   "source": [
    "## Embed Search Logits Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eef27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_optimisation.algorithms.embed_search import embed_search\n",
    "\n",
    "cfg = DotDict({\n",
    "    \"learn_rate\" : 0.0003,\n",
    "    # \"decay_rate\" : 1,\n",
    "    \"betas\" : (0.9,0.999),\n",
    "    # \"reset_epoch\" : 999999,\n",
    "    # \"reinit_epoch\" : 999999,\n",
    "    # \"bias_correction\" : True,\n",
    "    \"con_distance\" : {\n",
    "        0: \"cos sim\",\n",
    "        1: \"euc dist\",\n",
    "    }[0],\n",
    "    \"target_strategy\" : {\n",
    "        0: \"random\",\n",
    "        1: \"tinystories\",\n",
    "        2: \"reddit\",\n",
    "        3: \"wikipedia\",\n",
    "        4: \"privacy\",\n",
    "    }[0],\n",
    "    \"init_strategy\" : {\n",
    "        0: \"loaded\",\n",
    "        1: \"normal\",\n",
    "        2: \"zeros\",\n",
    "    }[2],\n",
    "    \"save_folder\": \"Embed_TinyStories33M\",\n",
    "    \"model_name\": \"tiny-stories-33M\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129dab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for dataset size\n",
    "cfg.input_len = 1\n",
    "cfg.num_targets = 1000\n",
    "cfg.max_epochs = 100 * 1000\n",
    "# cfg.max_chunk_size = [30,20,15,10,8,6,6,5,5,4][cfg.input_len-1] # L4 batch with 100 batch\n",
    "cfg.max_chunk_size = [20,15,10,7,6,5,4,3,2,2][cfg.input_len-1] # L4 batch with 1000 batch\n",
    "cfg.max_batch_size = [1000,1000,1000,1000,1000,1000,1000,1000,1000,1000][cfg.input_len-1] # L4 batch\n",
    "cfg.check_con_epochs = 200\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Generate the targets used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'true_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# # Generate the initialisations used for all experiments\n",
    "# tokens_list = []\n",
    "# for _ in tqdm(range(cfg.num_targets)):\n",
    "#     tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "#     tokens_list.append(tokens)\n",
    "# initial_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "# with open(DATA_PATH / f'initial_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "#     pickle.dump(initial_tokens, file)\n",
    "\n",
    "results, elapsed_time = embed_search(cfg, model, device)\n",
    "print()\n",
    "stats = get_paper_summary_stats_new(results, epochs=cfg.max_epochs)\n",
    "stats[\"elapsed_time\"] = elapsed_time\n",
    "stats[\"experiment_params\"] = cfg\n",
    "print(\"input_len\", cfg.input_len, \"\\tpercent_zero_loss\", stats[\"percent_zero_loss\"], \"\\tpercent_exact_inversion\", stats[\"percent_exact_inversion\"], \"\\telapsed_time\", stats[\"elapsed_time\"])\n",
    "\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/stats_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/results_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cf95c4",
   "metadata": {},
   "source": [
    "## PEZ Text-Only Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab0133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_optimisation.algorithms.pez import pez_text_search\n",
    "from inversion_optimisation.inv_configs import PEZ_TEXT_CFG\n",
    "\n",
    "cfg = PEZ_TEXT_CFG\n",
    "# cfg = DotDict({\n",
    "#     \"learn_rate\" : 2.5,\n",
    "#     \"con_distance\" : {\n",
    "#         0: \"cos sim\",\n",
    "#         1: \"euc dist\",\n",
    "#     }[1],\n",
    "#     \"target_sample\" : {\n",
    "#         0: \"random\",\n",
    "#         1: \"greedy\"\n",
    "#     }[1],\n",
    "#     \"target_strategy\" : {\n",
    "#         0: \"random\",\n",
    "#         1: \"tinystories\",\n",
    "#         2: \"reddit\",\n",
    "#         3: \"wikipedia\"\n",
    "#     }[0],\n",
    "#     \"init_strategy\" : {\n",
    "#         0: \"loaded\",\n",
    "#         1: \"normal\",\n",
    "#         2: \"zeros\",\n",
    "#     }[0],\n",
    "#     \"save_folder\": \"PEZ_TinyStories33M_text\",\n",
    "#     \"model_name\": \"tiny-stories-33M\",\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52659331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALISING STATE\n",
      "(0/100)(100/100)100, (0/100)(100/100)200, (1/100)(100/100)300, (2/100)(100/100)400, (2/100)(100/100)500, (3/100)(100/100)600, (5/100)(100/100)700, (6/100)(100/100)800, (6/100)(100/100)900, (6/100)(100/100)1000, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2557.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_len 1 \tpercent_zero_loss 6.0 \tpercent_exact_inversion 3.0 \telapsed_time 110.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set parameters for dataset size\n",
    "cfg.input_len = 1\n",
    "cfg.output_len = 25\n",
    "cfg.num_targets = 100\n",
    "cfg.max_chunk_size = 20\n",
    "cfg.max_epochs = 100 * 1000\n",
    "cfg.max_batch_size = {\n",
    "    \"tiny-stories-33M\" : [300,300,280,270,260,250,250,240,240,230],\n",
    "}[cfg.model_name][cfg.input_len-1]\n",
    "cfg.max_batch_size = cfg.max_batch_size - 15\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Generate the targets used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'true_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "batch_size = 1000\n",
    "for batch in range(0, cfg.num_targets, batch_size):\n",
    "    input_tokens = true_tokens[batch:batch+batch_size].to(device)\n",
    "    output_tokens = model.generate(\n",
    "        input_tokens,\n",
    "        # min_new_tokens=cfg.output_len,\n",
    "        max_new_tokens=cfg.output_len,\n",
    "        # do_sample=True,\n",
    "        do_sample=False,\n",
    "        stop_at_eos=False,\n",
    "        verbose=False,\n",
    "        return_type=\"tokens\",)[:,cfg.input_len:]\n",
    "    if batch == 0:\n",
    "        all_output_tokens = output_tokens\n",
    "    else:\n",
    "        all_output_tokens = torch.cat((all_output_tokens, output_tokens), dim=0)\n",
    "with open(DATA_PATH / f\"true_tokens_{cfg.num_targets}_{cfg.input_len}_{cfg.output_len}_greedy.pkl\", 'wb') as file:\n",
    "    pickle.dump(all_output_tokens, file)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Generate the initialisations used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "initial_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'initial_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(initial_tokens, file)\n",
    "\n",
    "results, elapsed_time = pez_text_search(cfg, model, device)\n",
    "print()\n",
    "stats = get_paper_summary_stats_new(results, epochs=cfg.max_epochs)\n",
    "stats[\"elapsed_time\"] = elapsed_time\n",
    "stats[\"experiment_params\"] = cfg\n",
    "print(\"input_len\", cfg.input_len, \"\\tpercent_zero_loss\", stats[\"percent_zero_loss\"], \"\\tpercent_exact_inversion\", stats[\"percent_exact_inversion\"], \"\\telapsed_time\", stats[\"elapsed_time\"])\n",
    "\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/stats_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/results_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3174551e",
   "metadata": {},
   "source": [
    "## PEZ Logits Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa88b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_optimisation.algorithms.pez import pez_search\n",
    "from inversion_optimisation.inv_configs import PEZ_LOGITS_CFG\n",
    "\n",
    "cfg = PEZ_LOGITS_CFG\n",
    "# cfg = DotDict({\n",
    "#     \"learn_rate\" : 1.7,\n",
    "#     \"con_distance\" : {\n",
    "#         0: \"cos sim\",\n",
    "#         1: \"euc dist\",\n",
    "#     }[1],\n",
    "#     \"target_strategy\" : {\n",
    "#         0: \"random\",\n",
    "#         1: \"tinystories\",\n",
    "#         2: \"reddit\",\n",
    "#         3: \"wikipedia\",\n",
    "#         4: \"privacy\",\n",
    "#     }[0],\n",
    "#     \"init_strategy\" : {\n",
    "#         0: \"loaded\",\n",
    "#         1: \"normal\",\n",
    "#         2: \"zeros\",\n",
    "#     }[0],\n",
    "#     \"save_folder\": \"PEZ_TinyStories33M\",\n",
    "#     \"model_name\": \"tiny-stories-33M\",\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14411b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 127.30it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 137.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALISING STATE\n",
      "(1/100)(100/100)100, (1/100)(100/100)200, (2/100)(100/100)300, (2/100)(100/100)400, (3/100)(100/100)500, (3/100)(100/100)600, (4/100)(100/100)700, (5/100)(100/100)800, (5/100)(100/100)900, (6/100)(100/100)1000, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 7202.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_len 3 \tpercent_zero_loss 6.0 \tpercent_exact_inversion 6.0 \telapsed_time 96.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set parameters for dataset size\n",
    "cfg.input_len = 2\n",
    "cfg.num_targets = 1000\n",
    "cfg.max_epochs = 100 * 1000\n",
    "# cfg.max_chunk_size = [30,20,15,10,8,6,6,5,5,4][cfg.input_len-1] # L4 batch with 100 batch\n",
    "cfg.max_chunk_size = [20,15,10,7,6,5,4,3,2,2][cfg.input_len-1] # L4 batch with 1000 batch\n",
    "cfg.max_batch_size = [1000,1000,1000,1000,1000,1000,1000,1000,1000,1000][cfg.input_len-1] # L4 batch\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Generate the targets used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'true_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Generate the initialisations used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "initial_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'initial_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(initial_tokens, file)\n",
    "\n",
    "results, elapsed_time = pez_search(cfg, model, device)\n",
    "print()\n",
    "stats = get_paper_summary_stats_new(results, epochs=cfg.max_epochs)\n",
    "stats[\"elapsed_time\"] = elapsed_time\n",
    "stats[\"experiment_params\"] = cfg\n",
    "print(\"input_len\", cfg.input_len, \"\\tpercent_zero_loss\", stats[\"percent_zero_loss\"], \"\\tpercent_exact_inversion\", stats[\"percent_exact_inversion\"], \"\\telapsed_time\", stats[\"elapsed_time\"])\n",
    "\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/stats_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/results_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b68e8",
   "metadata": {},
   "source": [
    "## Constrained PIA Text-Only Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7dc363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_optimisation.algorithms.constrained_pia import pia_text_search\n",
    "from inversion_optimisation.inv_configs import PIA_TEXT_CFG\n",
    "\n",
    "cfg = PIA_TEXT_CFG\n",
    "# cfg = DotDict({\n",
    "#     \"learn_rate\" : 3.5,\n",
    "#     \"top_k\" : 50256,\n",
    "#     \"nn_weight\" : 0.001,\n",
    "#     \"target_sample\" : {\n",
    "#         0: \"random\",\n",
    "#         1: \"greedy\"\n",
    "#     }[1],\n",
    "#     \"target_strategy\" : {\n",
    "#         0: \"random\",\n",
    "#         1: \"tinystories\",\n",
    "#         2: \"reddit\",\n",
    "#         3: \"wikipedia\"\n",
    "#     }[0],\n",
    "#     \"init_strategy\" : {\n",
    "#         0: \"loaded\",\n",
    "#         1: \"normal\",\n",
    "#         2: \"zeros\",\n",
    "#     }[1],\n",
    "#     \"save_folder\": \"PIA_TinyStories33M_text\",\n",
    "#     \"model_name\": \"tiny-stories-33M\",\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for dataset size\n",
    "cfg.input_len = 1\n",
    "cfg.output_len = 25\n",
    "cfg.num_targets = 1000\n",
    "cfg.max_chunk_size = 20\n",
    "cfg.max_epochs = 100 * 1000\n",
    "cfg.max_batch_size = {\n",
    "    \"tiny-stories-33M\" : [300,300,280,270,260,250,250,240,240,230],\n",
    "}[cfg.model_name][cfg.input_len-1]\n",
    "cfg.max_batch_size = cfg.max_batch_size - 15\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Generate the targets used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'true_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "batch_size = 1000\n",
    "for batch in range(0, cfg.num_targets, batch_size):\n",
    "    input_tokens = true_tokens[batch:batch+batch_size].to(device)\n",
    "    output_tokens = model.generate(\n",
    "        input_tokens,\n",
    "        # min_new_tokens=cfg.output_len,\n",
    "        max_new_tokens=cfg.output_len,\n",
    "        # do_sample=True,\n",
    "        do_sample=False,\n",
    "        stop_at_eos=False,\n",
    "        verbose=False,\n",
    "        return_type=\"tokens\",)[:,cfg.input_len:]\n",
    "    if batch == 0:\n",
    "        all_output_tokens = output_tokens\n",
    "    else:\n",
    "        all_output_tokens = torch.cat((all_output_tokens, output_tokens), dim=0)\n",
    "with open(DATA_PATH / f\"true_tokens_{cfg.num_targets}_{cfg.input_len}_{cfg.output_len}_greedy.pkl\", 'wb') as file:\n",
    "    pickle.dump(all_output_tokens, file)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Generate the initialisations used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "initial_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'initial_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(initial_tokens, file)\n",
    "\n",
    "results, elapsed_time = pia_text_search(cfg, model, device)\n",
    "print()\n",
    "stats = get_paper_summary_stats_new(results, epochs=cfg.max_epochs)\n",
    "stats[\"elapsed_time\"] = elapsed_time\n",
    "stats[\"experiment_params\"] = cfg\n",
    "print(\"input_len\", cfg.input_len, \"\\tpercent_zero_loss\", stats[\"percent_zero_loss\"], \"\\tpercent_exact_inversion\", stats[\"percent_exact_inversion\"], \"\\telapsed_time\", stats[\"elapsed_time\"])\n",
    "\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/stats_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/results_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a3782",
   "metadata": {},
   "source": [
    "## Constrained PIA Logits Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c336d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_optimisation.algorithms.constrained_pia import pia_search\n",
    "from inversion_optimisation.inv_configs import PIA_LOGITS_CFG\n",
    "\n",
    "cfg = PIA_LOGITS_CFG\n",
    "# cfg = DotDict({\n",
    "#     \"learn_rate\" : 3.5,\n",
    "#     \"top_k\" : 50256,\n",
    "#     \"nn_weight\" : 0.001,\n",
    "#     \"target_strategy\" : {\n",
    "#         0: \"random\",\n",
    "#         1: \"tinystories\",\n",
    "#         2: \"reddit\",\n",
    "#         3: \"wikipedia\"\n",
    "#     }[0],\n",
    "#     \"init_strategy\" : {\n",
    "#         0: \"loaded\",\n",
    "#         1: \"normal\",\n",
    "#         2: \"zeros\",\n",
    "#     }[1],\n",
    "#     \"save_folder\": \"PIA_TinyStories33M\",\n",
    "#     \"model_name\": \"tiny-stories-33M\",\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for dataset size\n",
    "cfg.input_len = 2\n",
    "cfg.num_targets = 1000\n",
    "cfg.max_epochs = 100 * 1000\n",
    "# cfg.max_chunk_size = [30,20,15,10,8,6,6,5,5,4][cfg.input_len-1] # L4 batch with 100 batch\n",
    "cfg.max_chunk_size = [20,15,10,7,6,5,4,3,2,2][cfg.input_len-1] # L4 batch with 1000 batch\n",
    "cfg.max_batch_size = [1000,1000,1000,1000,1000,1000,1000,1000,1000,1000][cfg.input_len-1] # L4 batch\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Generate the targets used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'true_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Generate the initialisations used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "initial_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'initial_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(initial_tokens, file)\n",
    "\n",
    "results, elapsed_time = pia_search(cfg, model, device)\n",
    "print()\n",
    "stats = get_paper_summary_stats_new(results, epochs=cfg.max_epochs)\n",
    "stats[\"elapsed_time\"] = elapsed_time\n",
    "stats[\"experiment_params\"] = cfg\n",
    "print(\"input_len\", cfg.input_len, \"\\tpercent_zero_loss\", stats[\"percent_zero_loss\"], \"\\tpercent_exact_inversion\", stats[\"percent_exact_inversion\"], \"\\telapsed_time\", stats[\"elapsed_time\"])\n",
    "\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/stats_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/results_{cfg.input_len}_{cfg.num_targets}_{cfg.max_epochs}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec59f6",
   "metadata": {},
   "source": [
    "## ARCA Text-Only Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89faa344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_optimisation.algorithms.arca import arca_text_search\n",
    "from inversion_optimisation.inv_configs import ARCA_TEXT_CFG\n",
    "\n",
    "cfg = ARCA_TEXT_CFG\n",
    "# cfg = DotDict({\n",
    "#     \"top_k\" : 64,\n",
    "#     \"num_grad_samples\" : 10,\n",
    "#     \"token_choice\" : {\n",
    "#         0: \"uniform\",\n",
    "#         1: \"weighted\",\n",
    "#     }[1],\n",
    "#     \"num_mutations\" : 1,\n",
    "#     \"target_sample\" : {\n",
    "#         0: \"random\",\n",
    "#         1: \"greedy\"\n",
    "#     }[1],\n",
    "#     \"target_strategy\" : {\n",
    "#         0: \"random\",\n",
    "#         1: \"tinystories\",\n",
    "#         2: \"reddit\",\n",
    "#         3: \"wikipedia\"\n",
    "#     }[0],\n",
    "#     \"init_strategy\" : {\n",
    "#         0: \"loaded\",\n",
    "#         1: \"zeros\",\n",
    "#     }[0],\n",
    "#     \"save_folder\": \"ARCA_TinyStories33M_text\",\n",
    "#     \"model_name\": \"tiny-stories-33M\",\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea9874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALISING STATE\n",
      "(33/100)(100/100)50, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2190.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_len 1 \tpercent_zero_loss 34.0 \tpercent_exact_inversion 14.0 \telapsed_time 53.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set parameters for dataset size\n",
    "cfg.output_len = 25\n",
    "cfg.input_len = 1\n",
    "cfg.num_targets = 1000\n",
    "cfg.num_candidates = (32*22)\n",
    "cfg.adjusted_max_epochs = (32*22)\n",
    "cfg.max_batch_size = {\n",
    "    \"tiny-stories-33M\" : [300,300,280,270,260,250,250,240,240,230],\n",
    "}[cfg.model_name][cfg.input_len-1]\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Generate the targets and initialisations used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'true_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "batch_size = 1000\n",
    "for batch in range(0, cfg.num_targets, batch_size):\n",
    "    input_tokens = true_tokens[batch:batch+batch_size].to(device)\n",
    "    output_tokens = model.generate(\n",
    "        input_tokens,\n",
    "        max_new_tokens=cfg.output_len,\n",
    "        do_sample=False,\n",
    "        stop_at_eos=False,\n",
    "        verbose=False,\n",
    "        return_type=\"tokens\",)[:,cfg.input_len:]\n",
    "    if batch == 0:\n",
    "        all_output_tokens = output_tokens\n",
    "    else:\n",
    "        all_output_tokens = torch.cat((all_output_tokens, output_tokens), dim=0)\n",
    "with open(DATA_PATH / f\"true_tokens_{cfg.num_targets}_{cfg.input_len}_{cfg.output_len}_greedy.pkl\", 'wb') as file:\n",
    "    pickle.dump(all_output_tokens, file)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f\"initial_tokens_{cfg.num_targets}_{cfg.input_len}.pkl\", 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "results, elapsed_time = arca_text_search(cfg, model, device)\n",
    "print()\n",
    "stats = get_paper_summary_stats_new(results, epochs=cfg.adjusted_max_epochs)\n",
    "stats[\"elapsed_time\"] = elapsed_time\n",
    "stats[\"experiment_params\"] = cfg\n",
    "print(\"input_len\", cfg.input_len, \"\\tpercent_zero_loss\", stats[\"percent_zero_loss\"], \"\\tpercent_exact_inversion\", stats[\"percent_exact_inversion\"], \"\\telapsed_time\", stats[\"elapsed_time\"])\n",
    "\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/stats_{cfg.input_len}_{cfg.num_targets}_{cfg.adjusted_max_epochs}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/results_{cfg.input_len}_{cfg.num_targets}_{cfg.adjusted_max_epochs}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b27ac8",
   "metadata": {},
   "source": [
    "## ARCA Logits Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_optimisation.algorithms.arca import arca_search\n",
    "from inversion_optimisation.inv_configs import ARCA_LOGITS_CFG\n",
    "\n",
    "cfg = ARCA_LOGITS_CFG\n",
    "# cfg = DotDict({\n",
    "#     \"top_k\" : 128,\n",
    "#     \"num_grad_samples\" : 10,\n",
    "#     \"token_choice\" : {\n",
    "#         0: \"uniform\",\n",
    "#         1: \"weighted\",\n",
    "#     }[0],\n",
    "#     \"num_mutations\" : 1,\n",
    "#     \"target_strategy\" : {\n",
    "#         0: \"random\",\n",
    "#         1: \"tinystories\",\n",
    "#         2: \"reddit\",\n",
    "#         3: \"wikipedia\",\n",
    "#         4: \"privacy\",\n",
    "#     }[0],\n",
    "#     \"init_strategy\" : {\n",
    "#         0: \"loaded\",\n",
    "#         1: \"zeros\",\n",
    "#     }[0],\n",
    "#     \"save_folder\": \"ARCA_TinyStories33M\",\n",
    "#     \"model_name\": \"tiny-stories-33M\",\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for dataset size\n",
    "cfg.input_len = 2\n",
    "cfg.num_targets = 1000\n",
    "cfg.num_candidates = (32*22)\n",
    "cfg.adjusted_max_epochs = (32*22)\n",
    "cfg.max_batch_size = {\n",
    "    \"tiny-stories-33M\" : [1000,1000,1000,1000,1000,1000,1000,900,750,650,None, None, None, None, 500, 500, 475, 450, 420, 400, 375, 350, 325, 300],\n",
    "}[cfg.model_name][cfg.input_len-1]\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Generate the targets and initialisations used for all experiments\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f'true_tokens_{cfg.num_targets}_{cfg.input_len}.pkl', 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "tokens_list = []\n",
    "for _ in tqdm(range(cfg.num_targets)):\n",
    "    tokens = torch.randint(0, len(model.tokenizer.vocab), (1, cfg.input_len)).to(device)\n",
    "    tokens_list.append(tokens)\n",
    "true_tokens = torch.cat(tokens_list, dim=0).to(device)\n",
    "with open(DATA_PATH / f\"initial_tokens_{cfg.num_targets}_{cfg.input_len}.pkl\", 'wb') as file:\n",
    "    pickle.dump(true_tokens, file)\n",
    "\n",
    "results, elapsed_time = arca_search(cfg, model, device)\n",
    "print()\n",
    "stats = get_paper_summary_stats_new(results, epochs=cfg.adjusted_max_epochs)\n",
    "stats[\"elapsed_time\"] = elapsed_time\n",
    "stats[\"experiment_params\"] = cfg\n",
    "print(\"input_len\", cfg.input_len, \"\\tpercent_zero_loss\", stats[\"percent_zero_loss\"], \"\\tpercent_exact_inversion\", stats[\"percent_exact_inversion\"], \"\\telapsed_time\", stats[\"elapsed_time\"])\n",
    "\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/stats_{cfg.input_len}_{cfg.num_targets}_{cfg.adjusted_max_epochs}.json', 'w') as f:\n",
    "#     json.dump(stats, f)\n",
    "# with open(DATA_PATH / f'{cfg.save_folder}/results_{cfg.input_len}_{cfg.num_targets}_{cfg.adjusted_max_epochs}.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
